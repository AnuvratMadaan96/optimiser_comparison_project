# Optimizer Comparison Project: Adam, RMSprop, and AdamW on KMNIST

This project compares the performance of three optimizers—Adam, RMSprop, and AdamW—on a feedforward fully connected neural network using the KMNIST dataset. The project uses PyTorch and includes features like k-fold cross-validation and hyperparameter search to evaluate and optimize the model performance.

## Project Structure

```
/optimizer_comparison_project/
│
├── data/
│   └── kmnist_data.py           # Script for loading and preprocessing the KMNIST dataset
│
├── models/
│   └── feedforward_nn.py        # Defines the architecture of the feedforward neural network
│
├── optimizers/
│   └── optimizer_comparison.py  # Script to initialize and run experiments with different optimizers
│
├── utils/
│   ├── train_eval.py            # Script with training, evaluation, and k-fold cross-validation logic
│   └── hyperparameter_search.py # Script for performing Grid Search or Random Search
│
├── results/
│   └── results.csv              # CSV to store and compare results (accuracy, loss, time, etc.)
│
├── main.py                      # Main script to run the project, compare optimizers, and collect results
│
└── README.md                    # Project description and instructions
```

## Dataset
The project uses the KMNIST dataset (Kuzushiji-MNIST), which contains grayscale images of 28x28 Japanese characters. The data is automatically downloaded using the torchvision library.

### Feedforward Neural Network Architecture
The neural network is a fully connected feedforward network with the following architecture:

	•	Input Layer: 784 neurons (28x28 images flattened into a vector)
	•	Hidden Layer 1: 128 neurons with ReLU activation
	•	Hidden Layer 2: 64 neurons with ReLU activation
	•	Output Layer: 10 neurons (one for each class) with Softmax activation

### Optimizers
We compare the following optimizers:

	•	Adam
	•	RMSprop
	•	AdamW

The results are evaluated based on:

	•	Training accuracy
	•	Validation accuracy
	•	Training loss
	•	Validation loss
	•	Training time

### Features
1.	K-Fold Cross-Validation: The project uses k-fold cross-validation to ensure robust performance evaluation.
2.	Hyperparameter Search: Option to implement Grid Search or Random Search for hyperparameter tuning.
3.	Performance Visualization: Graphs are plotted to compare the performance of different optimizers (accuracy, loss) over training epochs.

### Requirements
Ensure you have the following installed:

	•	Python 3.8+
	•	PyTorch
	•	Torchvision
	•	Matplotlib
	•	Pandas

To install the required dependencies, run:
!pip install torch torchvision matplotlib pandas

### Usage

#### Step 1: Clone the Repository

```bash
git clone https://github.com/AnuvratMadaan96/optimiser_comparison_project.git
cd optimizer_comparison_project
```

#### Step 2: Run the Project
Run the main.py script to train the model and compare the optimizers:

```bash
python main.py
```

This script will:

	•	Load the KMNIST dataset.
	•	Train a feedforward neural network using each optimizer.
	•	Perform k-fold cross-validation.
	•	Save the results (accuracy, loss, training time) in results/results.csv.
	•	Plot the performance graphs for each optimizer.
#### Step 3: Visualize Results
After the script completes, performance graphs will be displayed comparing:

	•	Training Accuracy over epochs for Adam, RMSprop, and AdamW.
	•	Validation Accuracy over epochs.
	•	Training Loss over epochs.
	•	Validation Loss over epochs.

You can also inspect the detailed results in results/results.csv.

### Hyperparameter Tuning
This Project is just optimizing the learning rate for each optimizer.
But other tuning parameters can also be added in this.

### Results
The results of this project will help you:

	•	Compare the effectiveness of different optimizers (Adam, RMSprop, and AdamW) in terms of accuracy, loss, and training time.
	•	Understand how different optimizers affect the convergence of your model.
	•	Visualize performance trends using the plots generated by matplotlib.

### Contributing
Feel free to contribute to this project by submitting a pull request or reporting issues.
